{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is added by sphinx-gallery\n",
    "# It can be customized to whatever you like\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning quantum nodes into Torch Layers\n",
    "=======================================\n",
    "\n",
    "::: {.meta}\n",
    ":property=\\\"og:description\\\": Learn how to create hybrid ML models in\n",
    "PennyLane using Torch :property=\\\"og:image\\\":\n",
    "<https://pennylane.ai/qml/_images/PyTorch_icon.png>\n",
    ":::\n",
    "\n",
    "::: {.related}\n",
    "tutorial\\_qnn\\_module\\_tf Turning quantum nodes into Keras Layers\n",
    ":::\n",
    "\n",
    "*Author: Tom Bromley --- Posted: 02 November 2020. Last updated: 28\n",
    "January 2021.*\n",
    "\n",
    "Creating neural networks in [PyTorch](https://pytorch.org/) is easy\n",
    "using the [nn module](https://pytorch.org/docs/stable/nn.html). Models\n",
    "are constructed from elementary *layers* and can be trained using the\n",
    "PyTorch API. For example, the following code defines a two-layer network\n",
    "that could be used for binary classification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/ec2-user/anaconda3/envs/Braket/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/ec2-user/anaconda3/envs/Braket/lib/python3.10/site-packages (from scikit-learn) (1.11.3)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "layer_1 = torch.nn.Linear(2, 2)\n",
    "layer_2 = torch.nn.Linear(2, 2)\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "layers = [layer_1, layer_2, softmax]\n",
    "model = torch.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What if we want to add a quantum layer to our model?** This is\n",
    "possible in PennyLane:\n",
    "`QNodes <../glossary/hybrid_computation>`{.interpreted-text role=\"doc\"}\n",
    "can be converted into `torch.nn` layers and combined with the wide range\n",
    "of built-in classical [layers](https://pytorch.org/docs/stable/nn.html)\n",
    "to create truly hybrid models. This tutorial will guide you through a\n",
    "simple example to show you how it\\'s done!\n",
    "\n",
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "A similar demo explaining how to\n",
    "`turn quantum nodes into Keras layers <tutorial_qnn_module_tf>`{.interpreted-text\n",
    "role=\"doc\"} is also available.\n",
    ":::\n",
    "\n",
    "Fixing the dataset and problem\n",
    "==============================\n",
    "\n",
    "Let us begin by choosing a simple dataset and problem to allow us to\n",
    "focus on how the hybrid model is constructed. Our objective is to\n",
    "classify points generated from scikit-learn\\'s binary-class\n",
    "[make\\_moons()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html)\n",
    "dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_moons\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Set random seeds\u001b[39;00m\n\u001b[1;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.1)\n",
    "y_ = torch.unsqueeze(torch.tensor(y), 1)  # used for one-hot encoded labels\n",
    "y_hot = torch.scatter(torch.zeros((200, 2)), 1, y_, 1)\n",
    "\n",
    "c = [\"#1f77b4\" if y_ == 0 else \"#ff7f0e\" for y_ in y]  # colours for each class\n",
    "plt.axis(\"off\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a QNode\n",
    "================\n",
    "\n",
    "Our next step is to define the QNode that we want to interface with\n",
    "`torch.nn`. Any combination of device, operations and measurements that\n",
    "is valid in PennyLane can be used to compose the QNode. However, the\n",
    "QNode arguments must satisfy additional `conditions\n",
    "<code/api/pennylane.qnn.TorchLayer>`{.interpreted-text role=\"doc\"}\n",
    "including having an argument called `inputs`. All other arguments must\n",
    "be arrays or tensors and are treated as trainable weights in the model.\n",
    "We fix a two-qubit QNode using the\n",
    "`default.qubit <code/api/pennylane.devices.default_qubit.DefaultQubit>`{.interpreted-text\n",
    "role=\"doc\"} simulator and operations from the\n",
    "`templates <introduction/templates>`{.interpreted-text role=\"doc\"}\n",
    "module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum simulator/QPU\n",
    "#dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "#dev = qml.device(\"default.qubit\", wires=4)\n",
    "#n_shots=1000\n",
    "n_shots=2000\n",
    "n_qubits=2\n",
    "#dev = qml.device(\"lightning.qubit\", wires=tot_qubits)\n",
    "dev = qml.device('braket.aws.qubit', device_arn='arn:aws:braket:eu-west-2::device/qpu/oqc/Lucy', wires=n_qubits, shots=n_shots)\n",
    "#dev = qml.device('braket.aws.qubit', device_arn='arn:aws:braket:::device/quantum-simulator/amazon/sv1', wires=n_qubits, shots=n_shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def qnode(inputs, weights):\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interfacing with Torch\n",
    "======================\n",
    "\n",
    "With the QNode defined, we are ready to interface with `torch.nn`. This\n",
    "is achieved using the `~pennylane.qnn.TorchLayer`{.interpreted-text\n",
    "role=\"class\"} class of the `~pennylane.qnn`{.interpreted-text\n",
    "role=\"mod\"} module, which converts the QNode to the elementary building\n",
    "block of `torch.nn`: a *layer*. We shall see in the following how the\n",
    "resultant layer can be combined with other well-known neural network\n",
    "layers to form a hybrid model.\n",
    "\n",
    "We must first define the `weight_shapes` dictionary. Recall that all of\n",
    "the arguments of the QNode (except the one named `inputs`) are treated\n",
    "as trainable weights. For the QNode to be successfully converted to a\n",
    "layer in `torch.nn`, we need to provide the details of the shape of each\n",
    "trainable weight for them to be initialized. The `weight_shapes`\n",
    "dictionary maps from the argument names of the QNode to corresponding\n",
    "shapes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_layers = 6\n",
    "weight_shapes = {\"weights\": (n_layers, n_qubits)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the `weights` argument of the QNode is trainable and has\n",
    "shape given by `(n_layers, n_qubits)`, which is passed to\n",
    "`~pennylane.templates.layers.BasicEntanglerLayers`{.interpreted-text\n",
    "role=\"func\"}.\n",
    "\n",
    "Now that `weight_shapes` is defined, it is easy to then convert the\n",
    "QNode:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this done, the QNode can now be treated just like any other\n",
    "`torch.nn` layer and we can proceed using the familiar Torch workflow.\n",
    "\n",
    "Creating a hybrid model\n",
    "=======================\n",
    "\n",
    "Let\\'s create a basic three-layered hybrid model consisting of:\n",
    "\n",
    "1.  a 2-neuron fully connected classical layer\n",
    "2.  our 2-qubit QNode converted into a layer\n",
    "3.  another 2-neuron fully connected classical layer\n",
    "4.  a softmax activation to convert to a probability vector\n",
    "\n",
    "A diagram of the model can be seen in the figure below.\n",
    "\n",
    "![](/demonstrations/qnn_module/qnn_torch.png){.align-center\n",
    "width=\"100.0%\"}\n",
    "\n",
    "We can construct the model using the\n",
    "[Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
    "API:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "clayer_1 = torch.nn.Linear(2, 2)\n",
    "clayer_2 = torch.nn.Linear(2, 2)\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "layers = [clayer_1, qlayer, clayer_2, softmax]\n",
    "model = torch.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model\n",
    "==================\n",
    "\n",
    "We can now train our hybrid model on the classification dataset using\n",
    "the usual Torch approach. We\\'ll use the standard\n",
    "[SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)\n",
    "optimizer and the mean absolute error loss function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "loss = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are more advanced combinations of optimizer and loss\n",
    "function, but here we are focusing on the basics.\n",
    "\n",
    "The model is now ready to be trained!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = torch.tensor(X, requires_grad=True).float()\n",
    "y_hot = y_hot.float()\n",
    "\n",
    "batch_size = 5\n",
    "batches = 200 // batch_size\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    list(zip(X, y_hot)), batch_size=5, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "epochs = 6\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for xs, ys in data_loader:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        loss_evaluated = loss(model(xs), ys)\n",
    "        loss_evaluated.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss_evaluated\n",
    "\n",
    "    avg_loss = running_loss / batches\n",
    "    print(\"Average loss over epoch {}: {:.4f}\".format(epoch + 1, avg_loss))\n",
    "\n",
    "y_pred = model(X)\n",
    "predictions = torch.argmax(y_pred, axis=1).detach().numpy()\n",
    "\n",
    "correct = [1 if p == p_true else 0 for p, p_true in zip(predictions, y)]\n",
    "accuracy = sum(correct) / len(correct)\n",
    "print(f\"Accuracy: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do? The model looks to have successfully trained and the\n",
    "accuracy is reasonably high. In practice, we would aim to push the\n",
    "accuracy higher by thinking carefully about the model design and the\n",
    "choice of hyperparameters such as the learning rate.\n",
    "\n",
    "Creating non-sequential models\n",
    "==============================\n",
    "\n",
    "The model we created above was composed of a sequence of classical and\n",
    "quantum layers. This type of model is very common and is suitable in a\n",
    "lot of situations. However, in some cases we may want a greater degree\n",
    "of control over how the model is constructed, for example when we have\n",
    "multiple inputs and outputs or when we want to distribute the output of\n",
    "one layer into multiple subsequent layers.\n",
    "\n",
    "Suppose we want to make a hybrid model consisting of:\n",
    "\n",
    "1.  a 4-neuron fully connected classical layer\n",
    "2.  a 2-qubit quantum layer connected to the first two neurons of the\n",
    "    previous classical layer\n",
    "3.  a 2-qubit quantum layer connected to the second two neurons of the\n",
    "    previous classical layer\n",
    "4.  a 2-neuron fully connected classical layer which takes a\n",
    "    4-dimensional input from the combination of the previous quantum\n",
    "    layers\n",
    "5.  a softmax activation to convert to a probability vector\n",
    "\n",
    "A diagram of the model can be seen in the figure below.\n",
    "\n",
    "![](/demonstrations/qnn_module/qnn2_torch.png){.align-center\n",
    "width=\"100.0%\"}\n",
    "\n",
    "This model can also be constructed by creating a new class that inherits\n",
    "from the `torch.nn`\n",
    "[Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) and\n",
    "overriding the `forward()` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class HybridModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clayer_1 = torch.nn.Linear(2, 4)\n",
    "        self.qlayer_1 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        self.qlayer_2 = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "        self.clayer_2 = torch.nn.Linear(4, 2)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.clayer_1(x)\n",
    "        x_1, x_2 = torch.split(x, 2, dim=1)\n",
    "        x_1 = self.qlayer_1(x_1)\n",
    "        x_2 = self.qlayer_2(x_2)\n",
    "        x = torch.cat([x_1, x_2], axis=1)\n",
    "        x = self.clayer_2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "model = HybridModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, let\\'s train the model to check if it\\'s working:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "epochs = 6\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for xs, ys in data_loader:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        loss_evaluated = loss(model(xs), ys)\n",
    "        loss_evaluated.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss_evaluated\n",
    "\n",
    "    avg_loss = running_loss / batches\n",
    "    print(\"Average loss over epoch {}: {:.4f}\".format(epoch + 1, avg_loss))\n",
    "\n",
    "y_pred = model(X)\n",
    "predictions = torch.argmax(y_pred, axis=1).detach().numpy()\n",
    "\n",
    "correct = [1 if p == p_true else 0 for p, p_true in zip(predictions, y)]\n",
    "accuracy = sum(correct) / len(correct)\n",
    "print(f\"Accuracy: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We\\'ve mastered the basics of constructing hybrid\n",
    "classical-quantum models using PennyLane and Torch. Can you think of any\n",
    "interesting hybrid models to construct? How do they perform on realistic\n",
    "datasets?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the author\n",
    "================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_braket",
   "language": "python",
   "name": "conda_braket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
